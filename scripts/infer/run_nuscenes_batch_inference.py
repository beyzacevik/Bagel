#!/usr/bin/env python3
"""Run Bagel batch inference on Bagel-formatted nuScenes parquet shards.

This utility consumes the parquet structure generated by
``scripts/convert_nuscenes_to_bagel.py`` and synthesises predictions for every
frame using Bagel's interleaved inferencer.  It mirrors the nuScenes split
layout so the resulting directory can be evaluated with
``scripts/eval/compute_interleaved_metrics.py`` without additional alignment.

The script streams the parquet shards in timestamp order, groups frames by
``scene_token`` and ``sensor`` (configurable) and reuses a configurable number of
previous frames as visual context for each prediction.  Context frames can come
from the ground-truth sequence (teacher forcing) or from the previously
predicted frames to enable autoregressive rollouts.

Example
-------
```bash
python scripts/infer/run_nuscenes_batch_inference.py \
    --model-path models/BAGEL-7B-MoT \
    --input-dir /data/bagel_nuscenes/val \
    --output-dir /results/bagel_generations/val \
    --context-frames 3 \
    --context-mode ground_truth
```

The command above reads the Bagel nuScenes validation shards, conditions every
prediction on the three previous ground-truth frames within the same
``scene_token``/``sensor`` sequence, and writes a mirrored directory structure
with generated PNG frames ready for evaluation.
"""

from __future__ import annotations

import argparse
import io
import json
import logging
import random
from collections import deque
from dataclasses import dataclass
from pathlib import Path
from typing import Deque, Dict, Iterable, Iterator, List, MutableMapping, Optional, Sequence, Tuple

import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
import torch
from PIL import Image

from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch
from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model

from data.data_utils import add_special_tokens
from data.transforms import ImageTransform
from inferencer import InterleaveInferencer
from modeling.autoencoder import load_ae
from modeling.bagel import Bagel, BagelConfig, Qwen2Config, Qwen2ForCausalLM, SiglipVisionConfig, SiglipVisionModel
from modeling.qwen2 import Qwen2Tokenizer

LOGGER = logging.getLogger("bagel.nuscenes.batch_infer")

DEFAULT_SEQUENCE_COLUMNS = ("scene_token", "sensor")
DEFAULT_ORDER_COLUMNS = ("timestamp",)
SHARD_SCHEMA = pa.schema(
    [
        ("image", pa.binary()),
        ("captions", pa.string()),
        ("scene_token", pa.string()),
        ("scene_name", pa.string()),
        ("sample_token", pa.string()),
        ("sample_data_token", pa.string()),
        ("sensor", pa.string()),
        ("timestamp", pa.int64()),
        ("metadata", pa.string()),
    ]
)


@dataclass
class SequenceFrame:
    """In-memory representation of a single parquet row."""

    data: MutableMapping[str, object]
    reference_image: Image.Image


def _set_seed(seed: int) -> None:
    if seed < 0:
        return
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def _decode_image(blob: bytes) -> Image.Image:
    with Image.open(io.BytesIO(blob)) as image:
        return image.convert("RGB")


def _normalise_value(value: object) -> object:
    if isinstance(value, bytes):
        return value.decode("utf-8")
    return value


def _iter_rows(parquet_dir: Path) -> Iterator[MutableMapping[str, object]]:
    parquet_paths = sorted(parquet_dir.glob("*.parquet"))
    if not parquet_paths:
        raise FileNotFoundError(f"No parquet files found under '{parquet_dir}'.")

    for parquet_path in parquet_paths:
        LOGGER.info("Loading shard: %s", parquet_path)
        parquet_file = pq.ParquetFile(parquet_path)
        for batch in parquet_file.iter_batches():
            columns = batch.schema.names
            arrays = [column.to_pylist() for column in batch.columns]
            for values in zip(*arrays):
                row = {name: value for name, value in zip(columns, values)}
                yield row


def _sorted_frames(
    rows: Iterable[MutableMapping[str, object]],
    order_columns: Sequence[str],
) -> List[SequenceFrame]:
    frames: List[SequenceFrame] = []
    for row in rows:
        image_blob = row["image"]
        if not isinstance(image_blob, (bytes, bytearray)):
            raise TypeError("Expected image column to contain bytes.")
        reference_image = _decode_image(image_blob)
        frames.append(SequenceFrame(row, reference_image))

    frames.sort(key=lambda frame: [_normalise_value(frame.data[col]) for col in order_columns])
    return frames


def _iter_sequences(
    parquet_dir: Path,
    sequence_columns: Sequence[str],
    order_columns: Sequence[str],
) -> Iterator[Tuple[Tuple[object, ...], List[SequenceFrame]]]:
    current_key: Optional[Tuple[object, ...]] = None
    current_rows: List[MutableMapping[str, object]] = []

    for row in _iter_rows(parquet_dir):
        key = tuple(_normalise_value(row[column]) for column in sequence_columns)
        if current_key is None:
            current_key = key
        if key != current_key:
            yield current_key, _sorted_frames(current_rows, order_columns)
            current_rows = []
            current_key = key
        current_rows.append(row)

    if current_key is not None and current_rows:
        yield current_key, _sorted_frames(current_rows, order_columns)


def _extract_caption(raw_caption: object) -> str:
    if raw_caption is None:
        return ""
    if isinstance(raw_caption, bytes):
        raw_caption = raw_caption.decode("utf-8")
    if isinstance(raw_caption, str):
        raw_caption = raw_caption.strip()
        if not raw_caption:
            return ""
        try:
            parsed = json.loads(raw_caption)
        except json.JSONDecodeError:
            return raw_caption
    else:
        parsed = raw_caption

    if isinstance(parsed, dict):
        for key in ("caption", "text", "prompt"):
            value = parsed.get(key)
            if isinstance(value, str):
                return value
    if isinstance(parsed, list) and parsed:
        first = parsed[0]
        if isinstance(first, dict):
            for key in ("caption", "text", "prompt"):
                value = first.get(key)
                if isinstance(value, str):
                    return value
        if isinstance(first, str):
            return first
    if isinstance(parsed, str):
        return parsed
    return ""


def _ensure_output_dir(output_dir: Path) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)


def _write_shard(
    records: List[Dict[str, object]],
    output_dir: Path,
    prefix: str,
    shard_index: int,
    row_group_size: Optional[int],
) -> None:
    if not records:
        return
    table = pa.Table.from_pylist(records, schema=SHARD_SCHEMA)
    output_path = output_dir / f"{prefix}_{shard_index:05d}.parquet"
    LOGGER.info("Writing shard with %d rows: %s", len(records), output_path)
    pq.write_table(table, output_path, compression="zstd", row_group_size=row_group_size)


def _image_to_png_bytes(image: Image.Image) -> bytes:
    buffer = io.BytesIO()
    image.save(buffer, format="PNG")
    return buffer.getvalue()


def _load_inferencer(
    model_path: Path,
    device: str,
    mode: str,
    max_memory: Optional[str],
) -> InterleaveInferencer:
    LOGGER.info("Loading Bagel weights from %s", model_path)
    llm_config = Qwen2Config.from_json_file(str(model_path / "llm_config.json"))
    llm_config.qk_norm = True
    llm_config.tie_word_embeddings = False
    llm_config.layer_module = "Qwen2MoTDecoderLayer"

    vit_config = SiglipVisionConfig.from_json_file(str(model_path / "vit_config.json"))
    vit_config.rope = False
    vit_config.num_hidden_layers -= 1

    vae_model, vae_config = load_ae(local_path=str(model_path / "ae.safetensors"))

    config = BagelConfig(
        visual_gen=True,
        visual_und=True,
        llm_config=llm_config,
        vit_config=vit_config,
        vae_config=vae_config,
        vit_max_num_patch_per_side=70,
        connector_act="gelu_pytorch_tanh",
        latent_patch_size=2,
        max_latent_size=64,
    )

    target_device = torch.device(device if device else ("cuda" if torch.cuda.is_available() else "cpu"))
    if target_device.type == "cuda" and torch.cuda.is_available():
        torch.cuda.set_device(target_device)

    with init_empty_weights():
        language_model = Qwen2ForCausalLM(llm_config)
        vit_model = SiglipVisionModel(vit_config)
        model = Bagel(language_model, vit_model, config)
        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)

    tokenizer = Qwen2Tokenizer.from_pretrained(str(model_path))
    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)

    vae_transform = ImageTransform(1024, 512, 16)
    vit_transform = ImageTransform(980, 224, 14)

    vae_device: torch.device = target_device

    if mode == "bf16":
        device_count = torch.cuda.device_count()
        if device_count == 0:
            raise RuntimeError("bf16 loading requires at least one CUDA device.")

        max_memory_dict = None
        if max_memory is not None:
            max_memory_dict = {i: max_memory for i in range(device_count)}
        else:
            max_memory_dict = {i: "80GiB" for i in range(device_count)}

        device_map = infer_auto_device_map(
            model,
            max_memory=max_memory_dict,
            no_split_module_classes=["Bagel", "Qwen2MoTDecoderLayer"],
        )
        same_device_modules = [
            "language_model.model.embed_tokens",
            "time_embedder",
            "latent_pos_embed",
            "vae2llm",
            "llm2vae",
            "connector",
            "vit_pos_embed",
        ]
        if device_count == 1:
            first_device = next(iter(device_map.values())) if device_map else str(target_device)
            for module_name in same_device_modules:
                device_map[module_name] = first_device
        else:
            first_device = device_map.get(same_device_modules[0], "cuda:0")
            for module_name in same_device_modules:
                if module_name in device_map:
                    device_map[module_name] = first_device
        vae_device = torch.device(first_device)
        model = load_checkpoint_and_dispatch(
            model,
            checkpoint=str(model_path / "ema.safetensors"),
            device_map=device_map,
            offload_folder="offload",
            offload_buffers=True,
            dtype=torch.bfloat16,
            force_hooks=True,
        ).eval()
    elif mode in {"nf4", "int8"}:
        if not torch.cuda.is_available():
            raise RuntimeError("Quantised loading requires a CUDA device.")
        if mode == "nf4":
            quant_config = BnbQuantizationConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=False,
                bnb_4bit_quant_type="nf4",
            )
        else:
            quant_config = BnbQuantizationConfig(
                load_in_8bit=True,
                torch_dtype=torch.float32,
            )
        model = load_and_quantize_model(
            model,
            weights_location=str(model_path / "ema.safetensors"),
            bnb_quantization_config=quant_config,
            device_map="auto",
            offload_folder="offload",
        ).eval()
        vae_device = target_device
    else:
        raise ValueError(f"Unsupported loading mode: {mode}")

    vae_model = vae_model.to(vae_device).eval()

    inferencer = InterleaveInferencer(
        model=model,
        vae_model=vae_model,
        tokenizer=tokenizer,
        vae_transform=vae_transform,
        vit_transform=vit_transform,
        new_token_ids=new_token_ids,
    )
    return inferencer


def _generate_sequence(
    sequence_frames: List[SequenceFrame],
    inferencer: InterleaveInferencer,
    context_frames: int,
    context_mode: str,
    default_prompt: str,
    inference_kwargs: Dict[str, object],
) -> Iterator[Tuple[MutableMapping[str, object], Image.Image]]:
    if context_frames > 0:
        context: Deque[Image.Image] = deque(maxlen=context_frames)
    else:
        context = deque()

    for frame in sequence_frames:
        prompt = _extract_caption(frame.data.get("captions")) or default_prompt
        inputs: List[object] = []
        inputs.extend(list(context))
        inputs.append(prompt)

        local_kwargs = dict(inference_kwargs)
        if not context:
            local_kwargs["image_shapes"] = frame.reference_image.size[::-1]

        outputs = inferencer.interleave_inference(inputs, **local_kwargs)
        generated_image = next(output for output in outputs if isinstance(output, Image.Image))
        yield frame.data, generated_image

        if context_frames <= 0:
            continue
        if context_mode == "ground_truth":
            context.append(frame.reference_image)
        elif context_mode == "predicted":
            context.append(generated_image)
        else:
            raise ValueError(f"Unsupported context mode: {context_mode}")


def run_inference(args: argparse.Namespace) -> None:
    logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s - %(message)s")
    _set_seed(args.seed)

    inferencer = _load_inferencer(Path(args.model_path), args.device, args.load_mode, args.max_memory)

    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    _ensure_output_dir(output_dir)

    sequence_columns = tuple(args.sequence_columns) if args.sequence_columns else DEFAULT_SEQUENCE_COLUMNS
    order_columns = tuple(args.order_columns) if args.order_columns else DEFAULT_ORDER_COLUMNS

    records: List[Dict[str, object]] = []
    shard_index = 0
    num_frames = 0

    inference_kwargs = dict(
        think=args.think,
        understanding_output=False,
        max_think_token_n=args.max_think_token_n,
        do_sample=args.do_sample,
        text_temperature=args.text_temperature,
        cfg_text_scale=args.cfg_text_scale,
        cfg_img_scale=args.cfg_img_scale,
        cfg_interval=[args.cfg_interval_start, args.cfg_interval_end],
        timestep_shift=args.timestep_shift,
        num_timesteps=args.num_timesteps,
        cfg_renorm_min=args.cfg_renorm_min,
        cfg_renorm_type=args.cfg_renorm_type,
        image_shapes=(args.height, args.width),
        enable_taylorseer=args.enable_taylorseer,
    )

    for sequence_key, frames in _iter_sequences(input_dir, sequence_columns, order_columns):
        LOGGER.info("Processing sequence %s with %d frames", sequence_key, len(frames))
        for row, generated_image in _generate_sequence(
            frames,
            inferencer,
            context_frames=args.context_frames,
            context_mode=args.context_mode,
            default_prompt=args.default_prompt,
            inference_kwargs=inference_kwargs,
        ):
            captions = row.get("captions")
            if isinstance(captions, bytes):
                captions = captions.decode("utf-8")

            metadata = row.get("metadata")
            if isinstance(metadata, bytes):
                metadata = metadata.decode("utf-8")

            record = {
                "image": _image_to_png_bytes(generated_image),
                "captions": captions or "",
                "scene_token": str(_normalise_value(row.get("scene_token", ""))),
                "scene_name": str(_normalise_value(row.get("scene_name", ""))),
                "sample_token": str(_normalise_value(row.get("sample_token", ""))),
                "sample_data_token": str(_normalise_value(row.get("sample_data_token", ""))),
                "sensor": str(_normalise_value(row.get("sensor", ""))),
                "timestamp": int(_normalise_value(row.get("timestamp", 0)) or 0),
                "metadata": metadata or "",
            }
            records.append(record)
            num_frames += 1

            if len(records) >= args.shard_size:
                _write_shard(records, output_dir, args.shard_prefix, shard_index, args.row_group_size)
                records.clear()
                shard_index += 1

            if args.max_frames and num_frames >= args.max_frames:
                LOGGER.info("Reached frame limit (%d). Stopping.", args.max_frames)
                break
        if args.max_frames and num_frames >= args.max_frames:
            break

    if records:
        _write_shard(records, output_dir, args.shard_prefix, shard_index, args.row_group_size)

    LOGGER.info("Generated %d frames across %d shards.", num_frames, shard_index + (1 if records else 0))


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Batch inference for Bagel nuScenes parquet datasets.")
    parser.add_argument("--model-path", type=str, required=True, help="Path to the BAGEL-7B-MoT weights directory.")
    parser.add_argument("--input-dir", type=str, required=True, help="Directory containing Bagel-formatted nuScenes shards.")
    parser.add_argument("--output-dir", type=str, required=True, help="Directory where generated shards will be written.")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--load-mode", choices=["bf16", "nf4", "int8"], default="bf16", help="Model loading strategy.")
    parser.add_argument(
        "--max-memory",
        type=str,
        default=None,
        help="Optional per-GPU memory budget passed to accelerate (e.g. '60GiB').",
    )
    parser.add_argument("--context-frames", type=int, default=3, help="Number of previous frames to use as visual context.")
    parser.add_argument(
        "--context-mode",
        choices=["ground_truth", "predicted"],
        default="ground_truth",
        help="Whether to seed the context queue with reference or generated frames.",
    )
    parser.add_argument("--default-prompt", type=str, default="Camera capture of an urban driving scene.")
    parser.add_argument("--shard-prefix", type=str, default="generated")
    parser.add_argument("--shard-size", type=int, default=1024)
    parser.add_argument("--row-group-size", type=int, default=None)
    parser.add_argument("--sequence-columns", nargs="*", default=None)
    parser.add_argument("--order-columns", nargs="*", default=None)
    parser.add_argument("--max-frames", type=int, default=None, help="Optional limit on the number of frames to generate.")
    parser.add_argument("--seed", type=int, default=42)

    parser.add_argument("--think", action="store_true", help="Enable think tokens during generation.")
    parser.add_argument("--do-sample", action="store_true", help="Enable sampling when generating text tokens.")
    parser.add_argument("--max-think-token-n", type=int, default=1000)
    parser.add_argument("--text-temperature", type=float, default=0.3)
    parser.add_argument("--cfg-text-scale", type=float, default=4.0)
    parser.add_argument("--cfg-img-scale", type=float, default=1.5)
    parser.add_argument("--cfg-interval-start", type=float, default=0.4)
    parser.add_argument("--cfg-interval-end", type=float, default=1.0)
    parser.add_argument("--timestep-shift", type=float, default=3.0)
    parser.add_argument("--num-timesteps", type=int, default=50)
    parser.add_argument("--cfg-renorm-min", type=float, default=0.0)
    parser.add_argument("--cfg-renorm-type", type=str, default="global")
    parser.add_argument("--enable-taylorseer", action="store_true")
    parser.add_argument("--height", type=int, default=1024)
    parser.add_argument("--width", type=int, default=1024)

    return parser.parse_args(argv)


if __name__ == "__main__":
    run_inference(parse_args())
